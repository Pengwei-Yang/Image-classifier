{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Four kinds of classifiers are separately shown below, SVM classifier is our best model."
      ],
      "metadata": {
        "id": "aXBknu4x6--0"
      },
      "id": "aXBknu4x6--0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.K-Nearest Neighbour"
      ],
      "metadata": {
        "id": "3kvvNKkQ7E6g"
      },
      "id": "3kvvNKkQ7E6g"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "879a52e9",
      "metadata": {
        "id": "879a52e9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Reading training data:\n",
        "data_td = pd.read_csv('./Input/train/train.csv') #Reading the file of training data\n",
        "d_ta_feature = data_td.loc[:, \"v1\":\"v784\"].to_numpy() #Take features of traning data\n",
        "d_ta_label = data_td.loc[:, 'label'].to_numpy() #Take labels of training data\n",
        "\n",
        "X_ta = d_ta_feature\n",
        "y_ta = d_ta_label\n",
        "\n",
        "#Normilisation for training data\n",
        "from sklearn.preprocessing import MinMaxScaler #Use min-max scaling method\n",
        "tool = MinMaxScaler()\n",
        "tool.fit(X_ta) #Calculate the min and the max value of the training data\n",
        "X_ta_n = tool.transform(X_ta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91af9852",
      "metadata": {
        "id": "91af9852"
      },
      "outputs": [],
      "source": [
        "#Grid search based on KNN\n",
        "'''\n",
        "Hyperparameters:\n",
        "n for the number of neighbour, p for different kinds of distance calculational methods\n",
        "In order to reduce the computational complexity,\n",
        "we choose n<=10, and we set interval 3(i.e. n=1,4,7,10)\n",
        "As for hyperparameter p: p=1 -> Manhattan distance, p=2 -> Euclidean distance\n",
        "'''\n",
        "param_grid = {'n_neighbors': list(range(1,11,3)), 'p': [1, 2]}\n",
        "#Call the function of gridsearch based on cross-validation\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "#Call the function of KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "#8-fold cross-validation\n",
        "gd_knn = GridSearchCV(KNeighborsClassifier(), param_grid, cv=8, return_train_score=False)\n",
        "gd_knn.fit(X_ta_n, y_ta)\n",
        "print(\"best hyperparameter: {}\".format(gd_knn.best_params_))\n",
        "#Best hyperparameter:{'n_neighbors': 4, 'p': 1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f90a3a04",
      "metadata": {
        "id": "f90a3a04"
      },
      "outputs": [],
      "source": [
        "#Reading test dataset\n",
        "data_td = pd.read_csv('./Input/test/test_input.csv') #Reading the file of test data\n",
        "d_te_feature = data_td.loc[:, \"v1\":\"v784\"].to_numpy() #Take features of test data\n",
        "X_te = d_te_feature\n",
        "\n",
        "#Normalisation for test data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "tool = MinMaxScaler()\n",
        "tool.fit(X_te)\n",
        "X_te_n = tool.transform(X_te)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a131ef4b",
      "metadata": {
        "id": "a131ef4b"
      },
      "outputs": [],
      "source": [
        "#Build KNN model based on best hyperparameters('n_neighbors': 4, 'p': 1)\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "model_knn = KNeighborsClassifier(n_neighbors=4, p=1)\n",
        "model_knn.fit(X_ta_n, y_ta) #Build KNN model using training data\n",
        "y_pd = model_knn.predict(X_te_n) #Use the best model predict labels of test data\n",
        "op = pd.DataFrame(y_pd, columns = ['label']) #Input values to a column which named label\n",
        "#Upload the file\n",
        "op.to_csv('./Output/test_output.csv', sep=\",\", float_format='%d', index_label=\"id\")\n",
        "'''\n",
        "RESULTS:\n",
        "Running time-> 6m20s(Local) 2m1s(Colaboratory)\n",
        "Accuracy    -> 0.84550\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Decision Tree"
      ],
      "metadata": {
        "id": "7zmc9P7V78hz"
      },
      "id": "7zmc9P7V78hz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31059d4f",
      "metadata": {
        "id": "31059d4f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Reading training data:\n",
        "data_td = pd.read_csv('./Input/train/train.csv') #Reading the file of training data\n",
        "d_ta_feature = data_td.loc[:, \"v1\":\"v784\"].to_numpy() #Take features of traning data\n",
        "d_ta_label = data_td.loc[:, 'label'].to_numpy() #Take labels of training data\n",
        "\n",
        "X_ta = d_ta_feature\n",
        "y_ta = d_ta_label\n",
        "\n",
        "#Normalisation for training data\n",
        "from sklearn.preprocessing import MinMaxScaler #Use min-max scaling method\n",
        "tool = MinMaxScaler()\n",
        "tool.fit(X_ta) #Calculate the min and the max value of the training data\n",
        "X_ta_n = tool.transform(X_ta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffc7bf3d",
      "metadata": {
        "id": "ffc7bf3d"
      },
      "outputs": [],
      "source": [
        "#Find the depth of the tree which is fully growth\n",
        "#In order to set the number in the step of grid search\n",
        "from sklearn.tree import DecisionTreeClassifier #Call the function of decision tree\n",
        "t = DecisionTreeClassifier(random_state=42)\n",
        "t.fit(X_ta_n, y_ta) #Build a fully growth decision tree model using training data\n",
        "print(t.get_depth())\n",
        "#Get the max_depth of tree-->37"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8ed4ac8",
      "metadata": {
        "id": "c8ed4ac8"
      },
      "outputs": [],
      "source": [
        "#Grid search based on DecisionTree\n",
        "'''\n",
        "Hyperparameters:\n",
        "1.criterion(i.e.calculation based on Gini impurity or information gain)\n",
        "2.max_depth->We already know the depth of fully growth tree is 37,\n",
        "consider of the computational complexity, we set interval as 5\n",
        "'''\n",
        "p_g = {'criterion': ['gini', 'entropy'], 'max_depth': range(2,38,5)}\n",
        "#Call the function of gridsearch based on cross-validation\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "#8-fold cross-validation\n",
        "gd_dt = GridSearchCV(DecisionTreeClassifier(), p_g, cv=8, return_train_score=False)\n",
        "gd_dt.fit(X_ta_n, y_ta)\n",
        "print(\"best hyperparameter: {}\".format(gd_dt.best_params_))\n",
        "#Best hyperparameter:{'criterion': 'entropy', 'max_depth': 12}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4416a9a",
      "metadata": {
        "id": "c4416a9a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "#Reading test dataset\n",
        "data_td = pd.read_csv('./Input/test/test_input.csv') #Reading the file of test data\n",
        "d_te_feature = data_td.loc[:, \"v1\":\"v784\"].to_numpy() #Take features of test data\n",
        "X_te = d_te_feature\n",
        "\n",
        "#Normalisation for test data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "tool = MinMaxScaler()\n",
        "tool.fit(X_te)\n",
        "X_te_n = tool.transform(X_te)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d09e574",
      "metadata": {
        "id": "8d09e574"
      },
      "outputs": [],
      "source": [
        "#Build DecisionTree model based on best hyperparameters{'criterion': 'entropy', 'max_depth': 12}\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model_DT = DecisionTreeClassifier(criterion='entropy', max_depth=12, random_state=42)\n",
        "model_DT.fit(X_ta_n, y_ta) #Build DecisionTree model using training data\n",
        "y_pd = model_DT.predict(X_te_n) #Use the best model predict labels of test data\n",
        "op = pd.DataFrame(y_pd, columns = ['label']) #Input values to a column which named label\n",
        "#Upload the file\n",
        "op.to_csv('./Output/test_output.csv', sep=\",\", float_format='%d', index_label=\"id\")\n",
        "'''\n",
        "RESULTS:\n",
        "Running time-> 14s(Local) 18s(Colaboratory)\n",
        "Accuracy    -> 0.81300\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Support Vector Machine【Our best model】"
      ],
      "metadata": {
        "id": "hE7S-nDO71yC"
      },
      "id": "hE7S-nDO71yC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a732cf86",
      "metadata": {
        "id": "a732cf86"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Reading training data:\n",
        "data_td = pd.read_csv('./Input/train/train.csv') #Reading the file of training data\n",
        "d_ta_feature = data_td.loc[:, \"v1\":\"v784\"].to_numpy() #Take features of traning data\n",
        "d_ta_label = data_td.loc[:, 'label'].to_numpy() #Take labels of training data\n",
        "\n",
        "X_ta = d_ta_feature\n",
        "y_ta = d_ta_label\n",
        "\n",
        "#Normalisation for training data\n",
        "from sklearn.preprocessing import MinMaxScaler #Use min-max scaling method\n",
        "tool = MinMaxScaler()\n",
        "tool.fit(X_ta) #Calculate the min and the max value of the training data\n",
        "X_ta_n = tool.transform(X_ta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c424b5f9",
      "metadata": {
        "id": "c424b5f9"
      },
      "outputs": [],
      "source": [
        "#Grid search based on SVM\n",
        "'''\n",
        "Hyperparameters:\n",
        "1.kernel->different kernel functions\n",
        "2.C for trading off the relative importance of maximizing the margin and fitting the training data.\n",
        "Large C: more emphasis on minimizing the training error than maximizing the margin.\n",
        "Consider of the computational complexity, we only choose three values of C :0.1,1,10.\n",
        "(The Latter one is ten times than the former one, which could allow a significant difference\n",
        "between these models)\n",
        "'''\n",
        "p_g = {'kernel': ['linear', 'poly', 'rbf'], 'C':[0.1, 1, 10]}\n",
        "#Call the function of gridsearch based on cross-validation\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC #Call the function of SVM\n",
        "grid_search = GridSearchCV(SVC(), p_g, cv=8, return_train_score=False) #8-fold cross-validation\n",
        "grid_search.fit(X_ta_n, y_ta)\n",
        "print(\"best hyperparameter: {}\".format(grid_search.best_params_))\n",
        "#Best hyperparameter: {'C': 10, 'kernel': 'rbf'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c98a06db",
      "metadata": {
        "id": "c98a06db"
      },
      "outputs": [],
      "source": [
        "#Reading test dataset\n",
        "data_td = pd.read_csv('./Input/test/test_input.csv')\n",
        "d_te_feature = data_td.loc[:, \"v1\":\"v784\"].to_numpy()\n",
        "X_te = d_te_feature\n",
        "\n",
        "#Normalisation for test data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "tool = MinMaxScaler()\n",
        "tool.fit(X_te)\n",
        "X_te_n = tool.transform(X_te)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b41ec508",
      "metadata": {
        "id": "b41ec508"
      },
      "outputs": [],
      "source": [
        "#Build SVM model based on best hyperparameters{'C': 10, 'kernel': 'rbf'}\n",
        "from sklearn.svm import SVC\n",
        "model_svm = SVC(C=10, kernel=\"rbf\")\n",
        "model_svm.fit(X_ta_n, y_ta) #Build SVM model using training data\n",
        "y_pd = model_svm.predict(X_te_n) #Use the best model predict labels of test data\n",
        "op = pd.DataFrame(y_pd, columns = ['label']) #Input values to a column which named label\n",
        "#Upload the file\n",
        "op.to_csv('./Output/test_output.csv', sep=\",\", float_format='%d', index_label=\"id\")\n",
        "'''\n",
        "RESULTS:\n",
        "Running time-> 2min(Local) 1m52s(Colaboratory)\n",
        "Accuracy    -> 0.89800\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.Random Forest(Based on PCA)"
      ],
      "metadata": {
        "id": "QfPtvnti7uUc"
      },
      "id": "QfPtvnti7uUc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ac5722c",
      "metadata": {
        "id": "6ac5722c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "#reading training and test data\n",
        "data_ta = pd.read_csv('./Input/train/train.csv') #Reading the file of training data\n",
        "y_ta = data_ta.loc[:, 'label'].to_numpy()\n",
        "data_te = pd.read_csv('./Input/test/test_input.csv') #Reading the file of test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07dcc153",
      "metadata": {
        "id": "07dcc153"
      },
      "outputs": [],
      "source": [
        "#PCA for training and test data\n",
        "from sklearn.decomposition import PCA #Call the function of PCA\n",
        "pca=PCA(n_components=0.95) #Choose 95% variance\n",
        "#Concatenate traning and test data\n",
        "all_da = pd.concat((data_ta.loc[:,'v1':'v784'],data_te.loc[:,'v1':'v784']))\n",
        "all_da_P = pca.fit_transform(all_da) #Using PCA to all data\n",
        "\n",
        "X_ta_P = all_da_P[:data_ta.shape[0]] #Extract training data\n",
        "X_te_P = all_da_P[data_ta.shape[0]:] #Extract test data\n",
        "print(\"Reduced shape of training data: {}\".format(str(X_ta_P.shape)))\n",
        "print(\"Reduced shape of test data: {}\".format(str(X_te_P.shape)))\n",
        "'''\n",
        "Print content:\n",
        "Reduced shape of training data: (30000, 187)\n",
        "Reduced shape of test data: (5000, 187)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19c705f0",
      "metadata": {
        "id": "19c705f0"
      },
      "outputs": [],
      "source": [
        "#Normalisation for training and test data\n",
        "from sklearn.preprocessing import MinMaxScaler #Use min-max scaling method\n",
        "tool = MinMaxScaler()\n",
        "tool.fit(X_ta_P) #Calculate the min and the max value of the training data\n",
        "X_ta_n = tool.transform(X_ta_P)\n",
        "tool.fit(X_te_P)\n",
        "X_te_n = tool.transform(X_te_P)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02c05d26",
      "metadata": {
        "id": "02c05d26"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Hyperparameters:\n",
        "1.n_estimators->the number of base classifiers.\n",
        "We set it start from 100 to expect a better model accuracy(compared with 1 or 10)\n",
        "2.max_features->different number of subset of features,\n",
        "namely sqrt[max_features=sqrt(n_features)] and log2[max_features=log2(n_features)].\n",
        "'''\n",
        "#Grid search for RandomForest\n",
        "p_g = {'n_estimators':[100,200,300,400,500], 'max_features':['sqrt','log2']}\n",
        "from sklearn.ensemble import RandomForestClassifier #Call the function of RandomForest\n",
        "#Call the function of gridsearch based on cross-validation\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "#8-fold cross-validation\n",
        "grid_search = GridSearchCV(RandomForestClassifier(), p_g, cv=8, return_train_score=False)\n",
        "grid_search.fit(X_ta_n, y_ta)\n",
        "print(\"best hyperparameter: {}\".format(grid_search.best_params_))\n",
        "#Best hyperparameter: {'max_features': 'sqrt', 'n_estimators': 400}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68c3f7cc",
      "metadata": {
        "id": "68c3f7cc"
      },
      "outputs": [],
      "source": [
        "#Build RandomForest model based on best hyperparameters{'max_features': 'sqrt', 'n_estimators': 400}\n",
        "#second time:3min40s\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model_RT = RandomForestClassifier(n_estimators=400, max_features='sqrt')\n",
        "model_RT.fit(X_ta_n,y_ta) #Build RandomForest model using training data\n",
        "y_pd = model_RT.predict(X_te_n) #Use the best model predict labels of test data\n",
        "op = pd.DataFrame(y_pd, columns = ['label']) #Input values to a column which named label\n",
        "#Upload the file\n",
        "op.to_csv('./Output/test_output.csv', sep=\",\", float_format='%d', index_label=\"id\")\n",
        "'''\n",
        "RESULTS:\n",
        "Running time-> 3m40s(Local) 3m40s(Colaboratory)\n",
        "Accuracy    -> 0.84600\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bff1681",
      "metadata": {
        "id": "7bff1681"
      },
      "outputs": [],
      "source": [
        "#Comparisions among different classifiers\n",
        "'''\n",
        "                       accuracy  Running time(Local)\n",
        "K-Nearest Neighbour    0.8455    6m20s\n",
        "Decision Tree          0.8130    14s\n",
        "Support Vector Machine 0.8980    2m\n",
        "Random Forest          0.8460    3m40s\n",
        "\n",
        "Description:\n",
        "It is apparent that Support Vector Machine has the best accuracy(0.8980) and less running time(2min).\n",
        "On the contrary, K-Nearest Neighbour, which has the longest running time among the classifiers,\n",
        "needs 6 minutes and 20 seconds for building a model and predicting labels.\n",
        "And the accuracy of K-Nearest Neighbour is 0.8455,\n",
        "which is similar to the accuracy of Random Forest(0.8460).\n",
        "Additionally, Decision Tree has the shortest running time(14s) and the worst accuracy(0.8130).\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea9dbeab",
      "metadata": {
        "id": "ea9dbeab"
      },
      "outputs": [],
      "source": [
        "#Hardware and software specifications:\n",
        "'''\n",
        "Local environment: CPU: Intel Core i7-6700 /GPU: Nvidia GeForce GTX 970M\n",
        "Colaboratory: GPU: K80\n",
        "Version of python：Python 3\n",
        "Used packages: Pandas, Sklearn\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}